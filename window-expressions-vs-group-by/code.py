# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1siwb1eYpnJP0tyrfLvwb4pY_lmKb4MG_
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.window import Window
import pyspark.sql.functions as F

spark = SparkSession.builder.appName("WindowExample").getOrCreate()

data = [(1, "Product A", 100),
 (2, "Product B", 150),
 (3, "Product C", 200),
 (4, "Product D", 120)]
columns = ["product_id", "product_name", "revenue"]
df = spark.createDataFrame(data, columns)

window_spec = Window.orderBy("product_id")
result = df.withColumn("total_revenue", F.sum("revenue").over(window_spec))
result.show()

window_spec = Window.partitionBy("product_name").orderBy("revenue")
result = df.withColumn("rank", F.rank().over(window_spec))
result.show()

window_spec = Window.orderBy("product_id").rowsBetween(-1, 1)
result = df.withColumn("moving_avg", F.avg("revenue").over(window_spec))
result.show()

import time

start_time = time.time()
grouped_df = df.groupBy("product_name").agg(F.sum("revenue"))
group_by_time = time.time() - start_time

start_time = time.time()
window_spec = Window.partitionBy("product_name")
windowed_df = df.withColumn("total_revenue", F.sum("revenue").over(window_spec))
window_time = time.time() - start_time

print(f"Time taken for GROUP BY: {group_by_time:.4f} seconds")
print(f"Time taken for Window Expression: {window_time:.4f} seconds")